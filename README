# Step 1 - Pre Requisites

First Install ArgoCD
    
    oc apply -k bootstrap/argocd 

    namespace/openshift-gitops created
    clusterrolebinding.rbac.authorization.k8s.io/argocd-rbac-ca created
    subscription.operators.coreos.com/openshift-gitops-operator created

Install all the needed operators:
    
    oc apply -f bootstrap/00_pre-requisites.yaml

    appproject.argoproj.io/pre-requisites created
    application.argoproj.io/pre-requisites created

Install Openshift AI:

    oc apply -f bootstrap/01_rhoai.yaml         

    appproject.argoproj.io/rhoai created
    application.argoproj.io/rhoai created


Now we deploy Minio for our S3 Storage:

    oc apply -f bootstrap/02_minio.yaml        

    appproject.argoproj.io/minio created
    application.argoproj.io/minio created

And finally we deploy the Elasticsearch for our vector database:

    oc apply -f bootstrap/04_elasticsearch.yaml

    application.argoproj.io/elasticsearch created

# Step 2 - Add the Certificates

Get the name of the ingress cert:

    oc get secrets -n openshift-ingress | grep cert

Extract from the secret:

    oc extract secret/<CERT_SECRET_FROM_ABOVE> -n openshift-ingress --to=aux/ingress-certs --confirm

Finally update the certs:

    cd aux/ingress-certs 

    oc create secret generic knative-serving-cert -n istio-system --from-file=. --dry-run=client -o yaml | oc apply -f -

    cd ../..

# Step 3 - Deploy the model Serving

    oc apply -k resources/model-server/components-serving
    
    dscinitialization.dscinitialization.opendatahub.io/default-dsci configured

# Step 4 - Deploy the llama model

Deploy custom-model-serving-runtime

Create the podman-ai-lab-rag-project data science project

Add serving runtime from the UI:

BUTTON: Add serving runtime

Select: Single-model serving platform

Add the: ./resources/custom-model-serving-runtime/llamacpp-runtime-custom.yaml

Now deploy the Model, start by filling the data: 

    Model name = mistral7b
    Serving runtime = LlamaCPP
    Model framework = any
    Deployment Mode = Standard
    Model server size = Medium
    Select the add external Route
    Select Create Connection => S3 Compatible
    Name = models
    Access key = minio
    Secret key = minio123
    Endpoint = Your Minio API URL
    Region = us-east-1
    Bucket = models
    Path = mistral7b

We'll now create a workbench where we can upload a Jupyter notebook to ingest data into the Elasticsearch vector database, we are going to do this in our Project (podman-ai-lab-rag-project)

    Select Create a workbench

Use the following values:

    elastic-vectordb-workbench
    Standard Data Science 
    Medium size

Add the following Enviroment Varibales using ConfigMap:

    CONNECTION_STRING: <Cluster-IP>:9200


You can get the values executing the following command:

    oc get service elasticsearch-sample-es-http -n elastic-vectordb

The other Enviroment Varibales is:

    PASSWORD: <Elasticsearch Password>

You can get the values executing the following command:

    oc get secret elasticsearch-sample-es-elastic-user -n elastic-vectordb -o jsonpath="{.data['elastic']}" | base64 -d > elastic_pass.txt

Note: You can delete the elastic_pass.txt file that you got the password from after you add it to the environment variable.

Now we need to log in to our workbench using the BUTTON on the Openshift-AI UI.

curl -k -u elastic:75od146Ee14f3hvifE1S9TUK https://172.30.47.209:9200